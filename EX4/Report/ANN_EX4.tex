\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks,plainpages=false]{hyperref}

\setlength\parindent{0pt}
\usepackage[english]{babel}
\usepackage[dvinames]{xcolor}
\usepackage[compact,small]{titlesec}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{marginnote}
\usepackage[top=1.8cm, bottom=1.8cm, outer=1.8cm, inner=1.8cm, heightrounded, marginparwidth=2.5cm, marginparsep=0.5cm]{geometry}
\usepackage{enumitem}
\setlist{noitemsep,parsep=2pt}
\newcommand{\highlight}[1]{\textcolor{kuleuven}{#1}}
\usepackage{pythonhighlight}
\usepackage{cleveref}
\usepackage{graphicx}
\graphicspath{{Pictures/}}
\usepackage{algorithmic}
\usepackage{tabularx}
\usepackage{bm}
\usepackage{subcaption}
\usepackage{array}


\newcommand{\nextyear}{\advance\year by 1 \the\year\advance\year by -1}
\newcommand{\thisyear}{\the\year}
\newcommand{\deadlineGroup}{November 27, \thisyear{} at 16:00 CET}
\newcommand{\deadlineCode}{December 18, \thisyear{} at 16:00 CET}
\newcommand{\deadlineReport}{January 4, \nextyear{} at 16:00 CET}

\newcommand{\ReplaceMe}[1]{{\color{blue}#1}}
\newcommand{\RemoveMe}[1]{{\color{purple}#1}}

\setlength{\parskip}{5pt}

%opening
\title{Artificial Neural Networks: Exercise session 4}
\author{Stijn Staring (r0620003)}

\begin{document}
\fontfamily{ppl}
\selectfont{}

\maketitle


\section{Restricted Boltzmann Machines}
The Boltzmann machine is a parametrized generative model representing a probability distribution. It is a neural network that belong to energy based models. A Boltzmann Machine consist out of a layer of visible units $ \bm{v} $ and hidden units $ \bm{h} $. The energy function used by the model is shown in Eq. \ref{eq:1a} and is part of the joint probability distribution. The standard type of Boltzmann Machines, uses binary-valued hidden and visible units and is therefore convenient to be used for black white images. The difference between a Restricted Boltzmann Machine and a conventional  Boltzmann Machine, is that the units of one layer are not connected with each other. Because of this restriction, the partition function in the joint probability distribution seen in Eq. \ref{eq:1b} becomes tractable. A RBM trains in an unsupervised manner using only the observations and calculating the gradient of the log-likelihood of the observations with respect to the weight matrix $ \bm{W} $, and bias terms $ \bm{a} $ and $ \bm{b} $. Training a RBM means adjusting the RBM weight values and  biases such that the probability distribution $ P(\bm{v}) $ fits the training data as well as possible. The gradient of the log-likelihood is calculated by the difference between the data expectation and model expectation and is used together with a learning rate and a gradient decent update strategy. However, because the calculation of the model expectation is intractable, the Contrastive Divergence algorithm is used. Gibbs sampling is applied to generate an observation from the model distribution and is used in the Contrastive Divergence algorithm. Gibbs sampling is initialized by a known observation

\begin{subequations}
	\begin{equation}\label{eq:1a}
		E(\bm{v},\bm{h}) = -\bm{v}^T\bm{W}\bm{h} - \bm{b}^T\bm{v}-\bm{a}^T\bm{h},
	\end{equation}
	\begin{equation}\label{eq:1b}
		P(\bm{v},\bm{h}) = \frac{1}{Z}e^{-E(\bm{v},\bm{h})}    .
	\end{equation}
	\label{eq:1}
\end{subequations}

\subsection{Hyperparameter tuning}
A small hyperparameter tuning is displayed in Table \ref{tab:pseudo-likelihood}. It is seen that more iterations give better results. When looking at the column with 20 iterations, an U-shape is noticed in the pseudo-likelihood. For a small learning rate of $ 0.005 $ a larger error is found due to the slow convergence and a larger learning rate of $ 0.1 $ performs worse due to oscillations during the application of gradient descent. The use of more hidden units gives an higher likelihood of the observations. Using more iterations and hidden units increases the calculation load of the model. The final chosen values for the parameters are $ 20 $ iterations, $ 20 $ hidden units and a learning rate of $ 0.01 $. In Figure \ref{fig:hyperpara} the comparison on the reconstruction of the number 4 is given with the tuned parameters and the default ones. It shows that the tuned RBM is able to better reconstruct an image with deleted row than when using the default parameter values that uses $ 10 $ iterations, $ 10 $ hidden units and a learning rate of $ 0.01 $ during training. 

\begin{table}[h]
	\begin{minipage}[t]{.55\linewidth}
		\centering
			\begin{tabular}[t]{@{}l|ccc}
				\firsthline
				\textbf{Learning rate}/\textbf{Iterations}	&  5 & 10 & 20 \\ \midrule
				0,005&	-141	&-134&	-127,41	\\ \hline
				0,01&	-166&	-147,66&	-125,55\\ \hline
				0,05 &	-141,15&	-134,35&	-127,41\\ \hline
				0,1 &	-134,46&	-130,24&	-130,81\\\bottomrule
			\end{tabular}		
	\end{minipage}
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\begin{tabular}[t]{@{}l|ccc}
			\firsthline
			\textbf{Iterations}/\textbf{Hidden units}	&  5 & 10 & 20 \\ \midrule
			5 &-185	&-155,08&	-134,46	\\ \hline
			10&	-186	&-154&	-130\\\bottomrule
		\end{tabular}
	\end{minipage} 
	\caption{Pseudo-likelihood on the training data when training with different parameter values.}
	\label{tab:pseudo-likelihood}
\end{table}

\subsection{Gibbs sampling}
In Gibbs sampling it is started from an initial known observation $ \bm{v}^{(1)} $ and by the sequential use of $ P(\bm{h}|\bm{v}^{(1)}) $ and $ P(\bm{v}|\bm{h}^{(1)}) $, one iteration of the Gibbs sampling is completed and a new observation is obtained. A property of Gibbs sampling is that only after a infinite amount of iterations it can be guaranteed that the generated sample originates from the model probability distribution that is tried to be sampled from. As can be seen in Figure \ref{fig:gibbssampeling} after one iteration the original number 2 is still visible but after 100 iterations it is not recognizable anymore. The influence of the initial observation is reduced after 100 iterations and the obtained observation will correspond better to one that  originates from the model probability distribution. 

\begin{figure}[ht]
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{0iter_gibbs.png}
		\caption{Original}
		\label{fig:0iter}
	\end{subfigure}	 	
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{1iter_gibbs.png}
		\caption{1 iteration}
		\label{fig:1iter}
	\end{subfigure}	
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{100iter_gibbs.png}
		\caption{100 iterations}
		\label{fig:100iter}
	\end{subfigure}
	\caption{The effect of iterations of Gibbs sampling.}
	\label{fig:gibbssampeling}
\end{figure}


\subsection{Reconstructing images}
The general rule in reconstructing images is that when more information is missing from the original image, it is harder to reconstruct. Deleting more information can be done by deleting more rows or deleting crucial places that play a key role in distinguishing the number. Figure \ref{fig:hyperpara} shows the reconstruction of the number 4 using the default RBM and tuned RBM when rows 10 till 15 are removed. The tuned model shows better results. In order to reconstruct the image, the image with deleted rows is feeded as observation to the trained RBM and Gibbs sampling is applied. An observation is retrieved from the model probability distribution and the corresponding rows are used to fill the gap of the original image. For both images 10 Gibbs sampling steps are used.

\begin{figure}[h]
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=0.4\linewidth]{default.png}
		\caption{Default}
		\label{fig:default}
	\end{subfigure}	 	
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=0.4\linewidth]{tuned.png}
		\caption{Tuned}
		\label{fig:tuned}
	\end{subfigure}	
	\caption{The effect of hyper parameter tuning.}
	\label{fig:hyperpara}
\end{figure}

\section{Deep Boltzmann Machines}
A Deep Boltzmann Machines is still restricted, which means that the nodes of the network of one layer are not connected. However, the difference with a shallow RBM is that it has multiple layers of hidden units stacked on top of each other. Due to this, a DBM can learn features that are non-linear combinations of features extracted in the lower layers. In comparison, the RBM can only extract features directly from the pixels. Therefore, more advanced features can be learned by a DBM. Figure \ref{fig:compareDBM} shows the weights of the hidden units of the RBM and two layers of the DBM. It is clear that both RBM and DBM show similar extracted features in layer 1. In the second layer more complex features are found that the RBM hasn't captured.

\begin{figure}[ht]
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{DBM_RBM.png}
		\caption{RBM}
		\label{fig:DBM_RBM}
	\end{subfigure}	 	
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{DBM_L1.png}
		\caption{DBM: L1}
		\label{fig:DBM_L1}
	\end{subfigure}	
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{DBM_L2.png}
		\caption{DBM: L2}
		\label{fig:DBM_L2}
	\end{subfigure}
	\caption{Weights of the units of a RBM and DBM.}
	\label{fig:compareDBM}
\end{figure}

That the DBM can better capture the important features of the digit is shown in Figure \ref{fig:compareDBM2} where it seen that the images originating from a DBM are much more clear. For the generation of the images 1 Gibbs sampling iteration is used. 

\begin{figure}[h]
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=0.4\linewidth]{DBM_RBM_gen.png}
		\caption{RBM}
		\label{fig:RBM_gen}
	\end{subfigure}	 	
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=0.4\linewidth]{DBM_gen.png}
		\caption{DBM}
		\label{fig:DBM_gen}
	\end{subfigure}	
	\caption{Generating digits with a RBM and DBM.}
	\label{fig:compareDBM2}
\end{figure}


\section{Generative Adversarial Networks}
A Generative adversarial network is an unsupervised learning task where regularities and patterns are learned in the input data. Afterwards the model can be used to generate examples that could have been part of the original training set. It is important that the generated examples are not exactly the same as the training examples because these are already available. A GAN should learn the underlying probability distribution of the training examples and sample from it to generate new examples. A GAN is trained making use of two adversarial neural networks that play a zero sum game against each other. Player one is the generative model that generates fake training examples from noise and player two is a discriminator that assesses if the presented example is a fake or real training example by assigning probabilities. When the discriminator is fooled around half of the times, training is stopped because this means that the generator generates plausible examples. By making use of the interplay between the two players, the learning problem is framed in a supervised learning problem. Neural networks used inside a GAN as generator is for example a CNN. Here no pooling is used because dimensionality reduction is not desired. Also no fully connected layers are used. The disadvantage of using GAN is that it can become unstable when learning in practise. Applications of GAN are:
\begin{itemize}
	\item Explainable AI
	\item Augementation of a dataset
\end{itemize}

When a model can generates logical data, the user puts more trust in it and it contributes to the explainability of the model. Also, GANs can be used in a context where there is only a small amount of data available and where it can be augmented with generated examples. A very famous example is the use of GAN in deepfakes where it is possible to generate a face on the movements of a person so that it looks like somebody else is doing or saying something in a video. \\

\begin{figure}[h]
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{birds.png}
		\caption{Generated bird images}
		\label{fig:birds}
	\end{subfigure}	 	
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=1.0\linewidth]{GAN_acc.png}
		\caption{Loss and accuracy of the generator vs discriminator}
		\label{fig:GAN_accuracy}
	\end{subfigure}	
	\caption{Generating digits with a RBM and DBM.}
	\label{fig:compareDBM2}
\end{figure}

Figure \ref{fig:birds} shows the generated examples of images of birds wherefore a GAN was trained on the CIFAR dataset. Figure \ref{fig:GAN_accuracy} shows the accuracy and loss of the generator in respectively yellow and red. The accuracy and loss of the discriminator are shown in respectively green and blue. It is observed that the four curves are correlated. This is logically because the Generator and Discriminator play a game against each other. When the Generator loss becomes higher, the Discriminator loss becomes lower. This can be clearly seen in the opposite direction of the red and bleu curves. The loss functions behave opposite to the accuracy and therefore yellow and red, and green and bleu also show opposite peaks. It can be seen that the accuracy and loss functions all converge which means that the model didn't become unstable. During training the effect of ``mode collapse'' could occur which means that the generator has found one image to always successfully mislead the discriminator with. Therefore, the generator starts converging to a situation where it always uses this particular image. The accuracy of the generator will become zero. 


\section{Optimal transport}
Optimal transport wants to change one probability distribution into another, while making minimal changes to the original one. To calculate distance between the two distributions, the Wasserstein distance is used. The Wasserstein distance measures the distance between two probability distributions by calculating the minimum of changes that have to be made to turn the first one into the second. 
During training of the optimal transport problem, this distance is tried to be minimized. If discrete distributions are considered, the Wasserstein  distance corresponds to the minimum of a linear program. A variant on the Wasserstein distance is the Sinkhorn distance where the amount of information entropy is added as a negative cost in the objective function of the distance metric and which serves as regulation term.

\subsection{Colour swapping}
Colour swapping of images is performed by creating a start set and a target set by randomly sampling respectively both the original pictures. Each sample corresponds to a random pixel and is characterized by three float numbers that correspond to a colour according to the RGB colour code. Because random sampling is performed all the samples in the start set and target set belong to a different probability distribution. The task during optimal transport is to learn the relation how to transform the start distribution to the target distribution. The function used to evaluate the learned relation between the two probability distributions are the Wasserstein and Sinkhorn distance. Results of the colour swapping are displayed in Figure \ref{fig:switch colour}. It can be seen that that the lightness of the colours are preserved causing the contours to be very visible. Using the Sinkhorn distance leads to a more homogenous colour distribution, which is in accordance to the highest amount of information entropy. When a non-optimal method is used, the contours of the original picture won't be preserved. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{OT_pictures.png}
	\caption{Swapping colour using OT. Pictures in order are: original, OT Wasserstein, OT Sinkhorn}
	\label{fig:switch colour}
\end{figure}

\subsection{WGAN}


\begin{figure}[ht]
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{WGAN2.png}
		\caption{WGAN: weight clipping}
		\label{fig:WGAN2}
	\end{subfigure}	 	
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{WGAN1.png}
		\caption{WGAN: Gradient penalization}
		\label{fig:WGAN1}
	\end{subfigure}	
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=0.5\linewidth]{GAN.png}
		\caption{GAN}
		\label{fig:GAN}
	\end{subfigure}
	\caption{Generated images from WGAN and GAN.}
	\label{fig:compareDBM}
\end{figure}



%\begin{table}
%	\centering
%	\begin{tabular}{@{}clr@{}} \toprule
%		\textbf{Attractor} & \textbf{Point} & \textbf{Stability}\\\midrule
%		Attractor $ 1 $ & $ [1;1] $ & Stable\\
%		Attractor $ 2 $ & $ [-1;-1] $ & Stable\\
%		Attractor $ 3 $ & $ [1;-1] $ & Stable\\
%		Attractor $ 4 $ & $ [-1;1] $ & Stable\\
%		Attractor $ 5 $ & $ [0;0] $ & Unstable\\
%		Attractor $ 6 $ & $ [0;1] $ & Unstable\\
%		Attractor $ 7 $ & $ [0;-1] $ & Unstable\\
%		Attractor $ 8 $ & $ [1;0] $ & Unstable\\
%		Attractor $ 9 $ & $ [-1;0] $ & Unstable\\\bottomrule
%	\end{tabular}
%	\caption{Overview of the different attractor points found in a 2D plane.}
%	\label{tab:att}
%\end{table}

\bibliographystyle{abbrv}
%\bibliography{ANN1}

\end{document}
